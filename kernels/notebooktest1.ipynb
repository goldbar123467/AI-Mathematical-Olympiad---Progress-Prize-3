{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":256586,"sourceType":"modelInstanceVersion","modelInstanceId":204046,"modelId":225262}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport io\nimport sys\nimport time\nimport signal\nimport torch\nfrom collections import Counter\nfrom contextlib import redirect_stdout\n\n\nclass CodeTimeout(Exception):\n    pass\n\n\ndef timeout_handler(signum, frame):\n    raise CodeTimeout()\n\nSTART_TIME = time.time()\n\nimport kaggle_evaluation.aimo_3_inference_server\nimport pandas as pd\nimport polars as pl\n\nMAX_TOKENS = 3072\nTEMPERATURE = 0.7\nNUM_SAMPLES = 8\nMAX_CODE_EXECUTIONS = 3  # Max code execution rounds per sample\nTIME_LIMIT_SECONDS = 4 * 3600 + 59 * 60  # 4:59:00 competition limit\n\nSYSTEM_PROMPT = \"\"\"You are a world-class mathematician solving International Mathematical Olympiad problems.\n\n## CRITICAL: USE PYTHON FOR CALCULATIONS\nWhen you need to compute anything non-trivial (modular arithmetic, large numbers, combinatorics, checking cases), write Python code in ```python blocks. The code will be executed and results returned to you.\n\nExample:\n```python\n# Calculate 2^100 mod 997\nresult = pow(2, 100, 997)\nprint(result)\n```\n\nDO NOT guess numerical results. ALWAYS compute them with code.\n\n## PROBLEM-SOLVING FRAMEWORK\n\n### Step 1: UNDERSTAND\n- What quantities are given?\n- What is being asked?\n- What type of problem is this? (geometry, number theory, combinatorics, algebra)\n\n### Step 2: EXPLORE WITH CODE\n- Write Python to try small cases\n- Use code to find patterns\n- Verify conjectures computationally\n\n### Step 3: SOLVE\n- Execute your approach step-by-step\n- Use Python for ALL calculations\n- State theorems you apply\n\n### Step 4: VERIFY WITH CODE\n- Write code to check your answer\n- Test edge cases\n- Confirm answer is in range [0, 99999]\n\n### Step 5: ANSWER\n- State your final answer\n- Put it in \\\\boxed{N} format\n\n## MATH CODE PATTERNS\n\n**Modular arithmetic:**\n```python\npow(base, exp, mod)  # Fast modular exponentiation\n```\n\n**Combinatorics:**\n```python\nfrom math import comb, factorial\ncomb(n, k)  # n choose k\n```\n\n**Number theory:**\n```python\nfrom math import gcd\nfrom functools import reduce\ndef lcm(a, b): return a * b // gcd(a, b)\n```\n\n**Brute force search:**\n```python\nfor n in range(1, 1000):\n    if condition(n):\n        print(n)\n        break\n```\n\n## CRITICAL\nYour final answer MUST be a single integer between 0 and 99999.\nExpress it as: \\\\boxed{YOUR_ANSWER}\"\"\"\n\n\ndef extract_code_blocks(text: str) -> list[str]:\n    \"\"\"Extract Python code blocks from model output.\"\"\"\n    pattern = r'```python\\n(.*?)```'\n    blocks = re.findall(pattern, text, re.DOTALL)\n    return blocks\n\n\ndef execute_code(code: str, timeout: int = 5) -> str:\n    \"\"\"Execute Python code with timeout and return output.\"\"\"\n    namespace = {\n        '__builtins__': __builtins__,\n        'math': __import__('math'),\n        'cmath': __import__('cmath'),\n        'itertools': __import__('itertools'),\n        'functools': __import__('functools'),\n        'fractions': __import__('fractions'),\n        'decimal': __import__('decimal'),\n        'collections': __import__('collections'),\n        'random': __import__('random'),\n        'sympy': None,\n    }\n\n    try:\n        namespace['sympy'] = __import__('sympy')\n    except ImportError:\n        pass\n\n    output = io.StringIO()\n\n    # Set timeout\n    old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n    signal.alarm(timeout)\n\n    try:\n        with redirect_stdout(output):\n            exec(code, namespace)\n        signal.alarm(0)  # Cancel alarm\n\n        result = output.getvalue().strip()\n\n        if not result:\n            for var in ['result', 'answer', 'ans', 'res']:\n                if var in namespace and namespace[var] is not None:\n                    result = str(namespace[var])\n                    break\n\n        return result if result else \"(code executed, no output)\"\n    except CodeTimeout:\n        return \"Error: Code execution timed out (5s limit)\"\n    except Exception as e:\n        signal.alarm(0)\n        return f\"Error: {type(e).__name__}: {str(e)}\"\n    finally:\n        signal.signal(signal.SIGALRM, old_handler)\n\n\ndef extract_answer(response: str) -> int:\n    \"\"\"Extract final numerical answer from response.\"\"\"\n    # Try \\boxed{N} first\n    boxed_pattern = r'\\\\boxed\\{(\\d+)\\}'\n    matches = re.findall(boxed_pattern, response)\n    if matches:\n        return int(matches[-1]) % 100000\n\n    # Fallback patterns\n    patterns = [\n        r'[Aa]nswer[:\\s]+(\\d+)',\n        r'[Ff]inal[:\\s]+(\\d+)',\n        r'\\*\\*(\\d+)\\*\\*',\n        r'= (\\d+)\\s*$'\n    ]\n    for pattern in patterns:\n        match = re.search(pattern, response, re.MULTILINE)\n        if match:\n            return int(match.group(1)) % 100000\n\n    # Last resort: find last number\n    lines = response.strip().split('\\n')\n    for line in reversed(lines):\n        numbers = re.findall(r'\\d+', line)\n        if numbers:\n            return int(numbers[-1]) % 100000\n    return 0\n\n\nMODEL_PATHS = [\n    \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b/2\",\n    \"/kaggle/input/deepseek-r1/deepseek-r1-distill-qwen-7b\",\n    \"/kaggle/input/deepseek-r1\",\n    \"/kaggle/input/deepseek-ai/deepseek-r1-distill-qwen-7b\",\n    \"/kaggle/input/deepseek-ai-deepseek-r1/transformers/deepseek-r1-distill-qwen-7b/2\",\n]\n\n\ndef find_model_path():\n    for path in MODEL_PATHS:\n        if os.path.exists(path):\n            # Check if it has model files\n            if os.path.isfile(os.path.join(path, \"config.json\")):\n                print(f\"Using model: {path}\")\n                return path\n\n    # Debug: list what's actually available\n    print(\"Model not found! Searching /kaggle/input/...\")\n    for root, dirs, files in os.walk(\"/kaggle/input\"):\n        if \"config.json\" in files:\n            print(f\"  Found model at: {root}\")\n        if root.count(os.sep) - \"/kaggle/input\".count(os.sep) > 3:\n            break  # Don't go too deep\n\n    raise FileNotFoundError(\"DeepSeek model not found. Check notebook inputs.\")\n\n\nclass Model:\n    def __init__(self):\n        self._model = None\n        self._tokenizer = None\n\n    def load(self):\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        model_path = find_model_path()\n        print(f\"Loading {model_path}...\")\n        self._tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, local_files_only=True\n        )\n        self._model = AutoModelForCausalLM.from_pretrained(\n            model_path, torch_dtype=torch.float16, device_map=\"auto\",\n            trust_remote_code=True, local_files_only=True\n        )\n        print(\"Model loaded!\")\n\n    def _generate_once(self, messages: list[dict]) -> str:\n        \"\"\"Generate a single response from messages.\"\"\"\n        prompt = self._tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        inputs = self._tokenizer(prompt, return_tensors=\"pt\").to(self._model.device)\n\n        with torch.no_grad():\n            outputs = self._model.generate(\n                **inputs,\n                max_new_tokens=MAX_TOKENS,\n                temperature=TEMPERATURE,\n                do_sample=True,\n                top_p=0.95,\n                pad_token_id=self._tokenizer.eos_token_id\n            )\n\n        response = self._tokenizer.decode(\n            outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True\n        )\n        return response\n\n    def _solve_with_tir(self, problem: str) -> tuple[str, int]:\n        \"\"\"Solve problem with Tool-Integrated Reasoning (code execution).\"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": problem}\n        ]\n\n        full_response = \"\"\n\n        for iteration in range(MAX_CODE_EXECUTIONS + 1):\n            response = self._generate_once(messages)\n            full_response += response\n\n            # Check for code blocks\n            code_blocks = extract_code_blocks(response)\n\n            # Early exit if we have a boxed answer and no pending code\n            has_answer = bool(re.search(r'\\\\boxed\\{\\d+\\}', response))\n            if not code_blocks or (has_answer and iteration > 0):\n                break\n\n            # Execute all code blocks\n            results = []\n            for i, code in enumerate(code_blocks):\n                result = execute_code(code)\n                results.append(f\"Code block {i+1} output:\\n{result}\")\n\n            execution_output = \"\\n\\n\".join(results)\n\n            # Add to conversation and continue\n            messages.append({\"role\": \"assistant\", \"content\": response})\n            messages.append({\n                \"role\": \"user\",\n                \"content\": f\"Execution results:\\n{execution_output}\\n\\nContinue solving using these results. When done, put your final answer in \\\\boxed{{N}} format.\"\n            })\n            full_response += f\"\\n\\n[CODE EXECUTED]\\n{execution_output}\\n\\n\"\n\n        answer = extract_answer(full_response)\n        return full_response, answer\n\n    def predict(self, problem: str) -> int:\n        if self._model is None:\n            self.load()\n\n        elapsed = time.time() - START_TIME\n        remaining = TIME_LIMIT_SECONDS - elapsed\n\n        # Adaptive sampling based on time remaining\n        if remaining < 300:  # Less than 5 min left\n            num_samples = 1\n            print(f\"URGENT: {remaining:.0f}s left, single sample mode\")\n        elif remaining < 900:  # Less than 15 min left\n            num_samples = 4\n            print(f\"WARNING: {remaining:.0f}s left, reduced to 4 samples\")\n        else:\n            num_samples = NUM_SAMPLES\n\n        answers = []\n\n        for sample_idx in range(num_samples):\n            try:\n                _, answer = self._solve_with_tir(problem)\n                answers.append(answer)\n            except Exception as e:\n                print(f\"Sample {sample_idx} error: {e}\")\n                answers.append(0)\n\n        # Majority vote with confidence tracking\n        counter = Counter(answers)\n        top_answer, top_count = counter.most_common(1)[0]\n        confidence = top_count / len(answers)\n\n        if confidence < 0.5:\n            print(f\"Low confidence ({confidence:.0%}): {dict(counter)}\")\n        else:\n            print(f\"Confident ({confidence:.0%}): answer={top_answer}\")\n\n        return top_answer\n\n\nmodel = Model()\n\n\ndef predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame | pd.DataFrame:\n    id_ = id_.item(0)\n    problem_text: str = problem.item(0)\n    try:\n        prediction = model.predict(problem_text)\n    except Exception as e:\n        print(f\"Error on {id_}: {e}\")\n        prediction = 0\n    return pl.DataFrame({'id': id_, 'answer': prediction})\n\n\ninference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}